{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __KNN__ (copilot response)\n",
    "\n",
    "__k-Nearest Neighbors__ (k-NN) is a non-parametric, lazy learning algorithm used for classification and regression. It operates by finding the k nearest neighbors to a query instance and making predictions based on the majority class (for classification) or average (for regression) of these neighbors.\n",
    "\n",
    "#### Steps in k-NN Algorithm:\n",
    "\n",
    "1. __Choose the number of neighbors (k):__ This is a `hyperparameter` that you set before running the algorithm.\n",
    "\n",
    "2. __Calculate the distance:__ Compute the distance between the query instance and `all` the training instances. Common distance metrics include Euclidean distance, Manhattan distance, etc.\n",
    "\n",
    "3. __Find the k nearest neighbors:__ Identify the k training instances that are closest to the query instance.\n",
    "\n",
    "4. __Make a prediction:__ For classification, the `majority` class among the `k neighbors` is the predicted class. For **regression**, the `average` value of the `k neighbors` is the predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (30, 4)\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris() \n",
    "X, y = iris.data, iris.target \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from KNN.Metrics import Metrics\n",
    "from KNN.KNN import KNN\n",
    "\n",
    "\n",
    "\n",
    "knn = KNN(k=3, distance_metric=Metrics.Lp, p=1) \n",
    "# print(X_train.shape, X_test.shape)\n",
    "knn.fit(X_train, y_train) \n",
    "predictions = knn.predict(X_test) \n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(predictions, y_test,sep=\"\\n\")\n",
    "print(accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Variants of k-NN Algorithm__\n",
    "\n",
    "1. __Weighted k-NN:__ \n",
    "        In the standard k-NN algorithm, each of the k nearest neighbors contributes equally to the prediction. In the weighted k-NN, each neighbor is weighted by its distance, with `closer neighbors having more influence` than distant ones.\n",
    "        $$ \\text{Weight}(d_i) = \\frac{1}{d_i + \\epsilon} $$\n",
    "        where $d_i$ is the distance of the i-th neighbor and $\\epsilon$ is a small value to avoid division by zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "knn = KNN(k=117, distance_metric=Metrics.Lp, weighted=True, p=1) \n",
    "# print(X_train.shape, X_test.shape)\n",
    "knn.fit(X_train, y_train) \n",
    "predictions = knn.predict(X_test) \n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(predictions, y_test,sep=\"\\n\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Distance-Weighted k-NN:__\n",
    "   \n",
    "    + In this variant, the contribution of each neighbor is weighted by the inverse of its distance, allowing closer neighbors to have a larger impact on the prediction.\n",
    "\n",
    "    $$\\hat{y} = \\frac{\\sum_{i=1}^k \\frac{y_i}{d_i}}{\\sum_{i=1}^k \\frac{1}{d_i}}$$\n",
    "\n",
    "    where $y_i$ is the value of the i-th neighbor and $d_i$ is its distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (30, 4) (120,)\n",
      "Accuracy: 0.77\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "[1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 2 1 1 2 0 1 0 2 1 1 1 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "from KNN.Metrics import Metrics\n",
    "from KNN.DWkNN import DWkNN\n",
    "\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape) \n",
    "knn = DWkNN(k=117, distance_metric=Metrics.Lp, p=2) \n",
    "# print(X_train.shape, X_test.shape)\n",
    "knn.fit(X_train, y_train) \n",
    "predictions = knn.predict(X_test)\n",
    "\n",
    "# print(f\"{array_preds:.2f}\")\n",
    "predictions = np.array([round(pred) for pred in predictions]) \n",
    "accuracy = np.mean(predictions == y_test)\n",
    "# print(accuracy)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print( y_test, predictions, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. __k-NN for Regression:__\n",
    "    \n",
    "    + In k-NN regression, the prediction is the average of the k nearest neighbors' values.\n",
    "\n",
    "    $$\\hat{y} = \\frac{1}{k} \\sum_{i=1}^k y_i$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560, 1) (140, 1) (560,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X, y = make_regression(n_samples=700, n_features=1, noise=3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "print(X_train.shape, X_test.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 69.61\n"
     ]
    }
   ],
   "source": [
    "from KNN.Metrics import Metrics\n",
    "from KNN.KNNRegressor import KNNRegressor\n",
    "\n",
    "knnReg = KNNRegressor(k=5, distance_metric=Metrics.Lp, p=2)\n",
    "knnReg.fit(X_train, y_train)\n",
    "predictions = knnReg.predict(X_test)\n",
    "\n",
    "mse = np.mean((predictions-y_test)**2)\n",
    "print(f\"MSE: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. __KNN For Regression__\n",
    "\n",
    "    + Weighted k-NN Regression:\n",
    "\n",
    "        $$w_i = \\frac{1}{d(x_q, x_i) + \\epsilon}$$\n",
    "\n",
    "    + Weighted Prediction:\n",
    "        $$ \\hat{y}_q = \\frac{\\sum_{i=1}^k w_i y_i}{\\sum_{i=1}^{k} w_i} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 37.15\n"
     ]
    }
   ],
   "source": [
    "from KNN.Metrics import Metrics\n",
    "from KNN.KNNRegressor import KNNRegressor\n",
    "\n",
    "knnReg = KNNRegressor(k=3, distance_metric=Metrics.Lp,weighted=True, p=2)\n",
    "knnReg.fit(X_train, y_train)\n",
    "predictions = knnReg.predict(X_test)\n",
    "\n",
    "mse = np.mean((predictions-y_test)**2)\n",
    "print(f\"MSE: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curse of Dimensionality in k-NN\n",
    "\n",
    "The __Curse of Dimensionality__ refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces. In the context of the k-Nearest Neighbors (k-NN) algorithm, it significantly impacts the performance and accuracy of the algorithm. As the __number of dimensions increases__, the __volume of the space increases exponentially__, making data points __sparse__ and the __distance between points less meaningful__.\n",
    "\n",
    "+ `Mathematical Explanation`\n",
    "    \n",
    "    1. __Distance Concentration:__ In high dimensions, the distance between any two points tends to become similar, making it difficult to distinguish between near and far points. This phenomenon can be demonstrated mathematically.\n",
    "        + For a point $x$ in a high-dimensional space, consider $d_i$ as the distance to the i-th nearest neighbor.\n",
    "        + As the number of dimensions $d$ increases, the ratio of the distance to the farthest point $d_{\\text{max}}$ to the nearest point $d_{\\text{min}}$ approaches 1:\n",
    "        $$ \\lim_{d \\to \\infty} \\frac{d_{\\text{max}}}{d_{\\text{min}}} \\to 1$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
