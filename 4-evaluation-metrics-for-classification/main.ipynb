{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "## 1. Confusion Matrix\n",
    "\n",
    "<table>\n",
    " <tr> \n",
    "    <td colspan=\"2\" rowspan=\"2\">Confusion Matrix</td>\n",
    "    <th colspan=\"4\">Predicted Label</th> \n",
    " </tr> \n",
    " <tr> \n",
    "    <th>Positive</th> \n",
    "    <th>Negative</th> \n",
    " </tr> \n",
    " <tr> \n",
    "    <th rowspan=\"4\">True Label</th> \n",
    "    <th>Positive</th> \n",
    "    <td>True Positive (TP)</td> \n",
    "    <td>False Positive (FP)</td> \n",
    " </tr> \n",
    " <tr> \n",
    "    <th>Negative</th> \n",
    "    <td>False Negative (FN)</td> \n",
    "    <td>True Negative (TN)</td> \n",
    " </tr> \n",
    " </table>\n",
    "\n",
    "## 2. Evaluation Metrics\n",
    "\n",
    "$$\n",
    "\\begin{align*} \n",
    "\\text{Precision} & = \\frac{TP}{TP + FP} \\\\ \n",
    "\\text{Recall} & = \\frac{TP}{TP + FN} \\\\ \n",
    "F1 & = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\\\ \\text{Accuracy} & = \\frac{TP + TN}{TP + FN + TN + FP} \\\\ \n",
    "\\text{Specificity} & = \\frac{TN}{TN + FP} \\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using scikit-Learn > metrics ,Model_selection\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_pred=, y_true=, normalize=\"true\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Seaborn to visaulize the confusion matrix\n",
    "\n",
    "1. statistical data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.heatmap(cm, annot=True, cmap='Blues', fmt=\".2%\")\n",
    "# plt.title(\"title\")\n",
    "# plt.show()\n",
    "\n",
    "# to use it, you must import pyplot\n",
    "# heatmap: accepts a matrix\n",
    "# annot: shows the number of each class\n",
    "# cmap: color map\n",
    "# fmt=\".2%\" : percision 2 decimal digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ROC: Resciver operating charactristic (Copilot response)\n",
    "\n",
    "\n",
    "__ROC__ stands for `Receiver Operating Characteristic`. An ROC curve is a graphical representation used to evaluate the __performance of a binary classification model__ by plotting the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings.\n",
    "\n",
    "+ True Positive Rate (TPR): Also known as Sensitivity or Recall, it is the ratio of correctly predicted positive observations to all actual positives.\n",
    "\n",
    "+ False Positive Rate (FPR): It is the ratio of incorrectly predicted positive observations to all actual negatives.\n",
    "\n",
    "### Interpreting an ROC Curve\n",
    "\n",
    "+ __X-axis (FPR):__ Ranges from 0 to 1, representing the False Positive Rate.\n",
    "\n",
    "+ __Y-axis (TPR):__ Ranges from 0 to 1, representing the True Positive Rate.\n",
    "\n",
    "+ __Diagonal Line:__ Represents a random classifier; the area under this line is 0.5, indicating no discriminative power.\n",
    "\n",
    "### Use of ROC Curves\n",
    "\n",
    "1. __Model Comparison:__\n",
    "\n",
    "    + ROC curves are particularly useful for comparing the performance of multiple models. A model with a curve closer to the top-left corner generally has a better performance.\n",
    "\n",
    "2. __Threshold Selection:__\n",
    "\n",
    "    + ROC curves help in selecting the optimal threshold for classification by visualizing the trade-off between TPR and FPR.\n",
    "\n",
    "3. __Performance Metric:__\n",
    "\n",
    "    + The Area Under the ROC Curve (AUC-ROC) is a single scalar value that summarizes the overall ability of the model to discriminate between positive and negative classes. An AUC-ROC value closer to 1 indicates a better performing model.\n",
    "\n",
    "### Meaning of AUC-ROC Values\n",
    "\n",
    "+ __AUC = 1:__ Perfect classifier.\n",
    "\n",
    "+ __0.5 < AUC < 1:__ Better than random guessing.\n",
    "\n",
    "+ __AUC = 0.5:__ Equivalent to random guessing.\n",
    "\n",
    "+ __AUC < 0.5:__ Worse than random guessing (could imply model is predicting opposite of the actual class).\n",
    "\n",
    "### Example of ROC Curve Use\n",
    "\n",
    "Suppose we have a binary classifier predicting whether an email is spam or not. By varying the decision threshold, we can plot the ROC curve to see how well the classifier distinguishes between spam and non-spam emails. A higher AUC-ROC value would indicate a better spam detection model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# fpr, tpr, _ = roc_curve(y_true=,y_score=)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
